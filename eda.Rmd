---
title: "STAT 151A Final Project"
author: "Benjamin Lee, Stephanie Trinh, Zhi Long Yeo"
date: "2023-04-05"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(dbplyr)
library(caret)
library(pracma)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
library(ggpubr)

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

# Modeling Question

## Overview

We plan to analyze various datasets related to bike sharing data from different geographical locations (e.g. Seoul, Washington D.C., London etc) which contains information about the number of bikes rented on different days, along with weather conditions (temperature, humidity, etc) and miscellaneous information about the day these bikes were rented (weekends, holidays, etc).

## Research question

The question we wish to answer using these datasets, is what various variables/factors can be used to predict the number of bikes that will be rented on a given day.

We also want to test the following hypotheses:

-   Is $\beta_{temp}$ different in different locations? We expect people in different areas to have different temperature preferences.
-   How does $\beta_{isWorkday}$ differ in different locations?
-   Which is the most significant $\beta$? We expect it to be $\beta_{temp}$, but expect $\beta_{windspeed}$ to be significant too.

## Practical Decisions

Our findings could help inform bike sharing companies into making better economic decisions.

i.  Marketing decisions: A better understanding of the factors that affect number of users can provide valuable insights into marketing decisions, such as discounts and promotions. Companies can offer discounts during off-peak times or inclement weather to encourage greater usage.

ii. Maintenance: Our findings could help bike sharing companies predict the expected mileage (and thus wear-and-tear) on the bikes ahead of time. It can also help them in finding the right timeslots (i.e a period where bike demand is not too high) to conduct their maintenance. Both of these factors aid them in scheduling their maintenance.

## Primary focus

Our primary focus is prediction accuracy. We will not be focusing on causal inference as we would have to control for confounding variables, and these confounding variables may not be captured in our datasets.

\newpage

# Data Overview

We are using the following datasets (each bullet-point is a hyperlink) for data exploration and modeling:

-   [Bike Sharing in Washington D.C. Dataset (2011-2012)](https://www.kaggle.com/datasets/marklvl/bike-sharing-dataset)
-   [Seoul Bike Sharing Demand Data Set (2017-2018)](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#)
-   [London bike sharing dataset (2015-2017)](https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset)

Each dataset contains the hourly count of rental bikes on each specific date, with additional information on weather and holiday schedules. Each observation corresponds to an hour of the day, resulting in observations being dependent on each other. Working with time series data will be one of the challenges of working with these datasets for linear modeling, but we plan to lessen the effects of dependence between observations by treating "hour" as a categorical variable.

It would probably be difficult for modeling on this data to be generalizable to a larger population and be applicable to other locations since the popularity of bike sharing and general trends varies across locations in a way that cannot be captured within the model. Working with these datasets will likely only provide us with a model appropriate specifically for Washington D.C, Seoul, and London.

Regarding additional features and data that we believe would be useful for analysis and modeling, we think that the inclusion of additional weather data like precipitation could be useful in improving the model. This is because it would make sense for the amount of rainfall to impact the number of people choosing to utilize rental bikes on a particular day. However, since not all the datasets we are using contain precipitation data, we are not currently planning to utilize rainfall as a feature in developing our model.

\newpage

# EDA

## Data Cleaning

Before creating the following visualizations, we performed some preprocessing steps to clean up the data. Since we planned to use three different datasets for bike rentals in three locations, we first selected only columns in each dataset that were readily available in all other datasets and derived certain columns that weren't explicitly available (e.g. extracting hour data and workday data from timestamps). Some datasets required additional cleaning (e.g. un-normalizing temperature data in the Washington D.C. dataset, ensuring that all datasets used the same units of measurement for windspeed and defined the same months per season). We then standardized how categorical variables were encoded using logicals to make it easier in the future for modeling to treat those columns as categorical data. After ensuring all the datasets followed the same format, we merged the datasets together.

## Distribution and Heteroscedasticity of Data

```{r}
df = read.csv('combined_dataset_cleaned.csv', colClasses= c("integer", "character", "logical", "character", "numeric", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "character", "integer", "logical", "logical"))

london_df = df[df$location == "london", ]
washington_df = df[df$location == "washington_dc", ]
seoul_df = df[df$location == "seoul", ]

select_by_location_hour_workday = function(df, location, hour, workday) {
  output = df[(df$location == location) & (df$hr == hour) & (df$is_workday == workday), ]
  output$cnt = output$cnt / as.numeric(max(output$cnt))
  return(output)
}

plot_by_location_hour_workday = function(df, location, hour, workday) {
  restricted_df = select_by_location_hour_workday(df, location, hour, workday)
  output = ggplot(restricted_df, aes(x=temp, y=cnt, color=season)) +
    geom_point() + 
    theme(legend.position='bottom', legend.direction = 'horizontal')
  return(output)
}
```

```{r, fig.width=36, fig.height=24, fig.align='center'}
boxplots <- ggplot(df, aes(x = month, y = cnt))

boxplots <- boxplots + 
  geom_boxplot(
    aes(x=month, y=cnt, group=month), 
    fill = "#00BFC4", 
    color = "black", 
    alpha = 0.8, 
    outlier.color = "#F8766D", 
    outlier.size = 0.7
    ) + 
  xlab("Month") + 
  ylab("Count") + 
  theme(plot.title = element_text(hjust = 0.5, face="bold"),
      plot.subtitle = element_text(hjust = 0.5),
      axis.title = element_text(face="bold"),
      text = element_text(size=50),
      panel.spacing.x = unit(25, "mm")) +
  facet_wrap(~location, scales = "free") + 
  scale_x_continuous(breaks=1:12)

boxplots_log <- ggplot(df, aes(x = month, y = log(cnt)))

boxplots_log <- boxplots_log + 
  geom_boxplot(
    aes(x=month, y=log(cnt), group=month), 
    fill = "#00BFC4", 
    color = "black", 
    alpha = 0.8, 
    outlier.color = "#F8766D", 
    outlier.size = 0.7
    ) + 
  xlab("Month") + 
  ylab("log(Count)") + 
  facet_wrap(~location, scales = "free") + 
  theme(plot.title = element_text(hjust = 0.5, face="bold"),
      plot.subtitle = element_text(hjust = 0.5),
      axis.title = element_text(face="bold"),
      text = element_text(size=50),
      panel.spacing.x = unit(25, "mm")) +
  scale_x_continuous(breaks=1:12)

grid.arrange(boxplots, boxplots_log, nrow=2)
```

From the top row of boxplots we observe that the average count and, in particular, variance of bike users count differs across the months. The non-uniform variance violates the canonical assumption of homoscedasticity. Furthermore, we also observe that the distribution of count in each month does not appear to be normal, as evident from the presence of numerous outliers at high values of count. In the second row, we make an attempt to normalize the data using a log-transformation in an attempt to pull in high values of count, but our resulting boxplots still show that the distribution is not normal.

These observations motivate treating the data as count data and working from a Poisson GLM framework. A Poisson framework would, in particular, address the non-uniform variance of our data. We observe that variance of count in each month increases as mean increases, which is characteristic of Poisson processes.

## Correlation Matrix of variables

```{r, fig.width=36, fig.height=24, fig.align='center'}

london1 = select_by_location_hour_workday(df, "london", 8, TRUE)
london2 = select_by_location_hour_workday(df, "london", 12, TRUE)
london3 = select_by_location_hour_workday(df, "london", 18, TRUE)

dc1 = select_by_location_hour_workday(df, "washington_dc", 8, TRUE)
dc2 = select_by_location_hour_workday(df, "washington_dc", 12, TRUE)
dc3 = select_by_location_hour_workday(df, "washington_dc", 18, TRUE)

seoul1 = select_by_location_hour_workday(df, "seoul", 8, TRUE)
seoul2 = select_by_location_hour_workday(df, "seoul", 12, TRUE)
seoul3 = select_by_location_hour_workday(df, "seoul", 18, TRUE)

select_continous_variables = function(df) {
  return(select(df, cnt, temp, hum, windspeed))
}

london_cor1 = cor(select_continous_variables(london1))
london_cor2 = cor(select_continous_variables(london2))
london_cor3 = cor(select_continous_variables(london3))

dc_cor1 = cor(select_continous_variables(dc1))
dc_cor2 = cor(select_continous_variables(dc2))
dc_cor3 = cor(select_continous_variables(dc3))

seoul_cor1 = cor(select_continous_variables(seoul1))
seoul_cor2 = cor(select_continous_variables(seoul2))
seoul_cor3 = cor(select_continous_variables(seoul3))

color_palette <- colorRampPalette(rev(RColorBrewer::brewer.pal(9, "YlOrRd")))
legend_limits <- c(-1, 1)
continuous_variables = c("cnt", "temp", "hum", "windspeed")

get_heatmap_plot = function(mat, title) {
  x <- colnames(as.data.frame(dc_cor1))
  y <- colnames(as.data.frame(dc_cor1))
  data <- expand.grid(X=x, Y=y)
  data$corr <- melt(as.data.frame(dc_cor1))$value
  
  output_plot = ggplot(
      data,
      aes(X, Y, fill=corr)) + 
    geom_tile() +
    ggtitle(title) +
    theme(legend.position = "bottom", 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(hjust = 0.5, size=40),
          axis.title = element_text(face="bold"),
          text = element_text(size=50),
          panel.spacing.x = unit(25, "mm"),
          legend.key.size = unit(25, "mm"),
          legend.key.height = unit(25, "mm"), #change legend key height
          legend.key.width = unit(100, "mm"), #change legend key width
          )
  return(output_plot)
}

ggarrange(
  get_heatmap_plot(london_cor1, "london 0800"),
  get_heatmap_plot(london_cor2, "london 1200"),
  get_heatmap_plot(london_cor3, "london 1800"),
  get_heatmap_plot(seoul_cor1, "seoul 0800"),
  get_heatmap_plot(seoul_cor2, "seoul 1200"),
  get_heatmap_plot(seoul_cor3, "seoul 1800"),
  get_heatmap_plot(dc_cor1, "washington_dc 0800"),
  get_heatmap_plot(dc_cor2,"washington_dc 1200"),
  get_heatmap_plot(dc_cor3,"washington_dc 1800"),
  common.legend = TRUE,
  legend = "bottom",
  nrow=3,
  ncol=3)
```

Conditioning on time and location, we see that the absolute value of the pairwise correlation of the covariates $temp$, $hum$ and $windspeed$ are all $\leq 0.3$, so we do not run into multicollinearity problems with this choice of covariates. In the following visualizations, we introduce motivation for including interaction terms, but acknowledge that the inclusion of such terms might introduce multicollinearity.

## Bike Rental Trends Given Season and Location

```{r, fig.width=36, fig.height=24, fig.align='center'}
library(ggpubr)

selected_hr = 18
is_weekend = 0

london1 = select_by_location_hour_workday(df, "london", 8, TRUE)
london2 = select_by_location_hour_workday(df, "london", 12, TRUE)
london3 = select_by_location_hour_workday(df, "london", 18, TRUE)

dc1 = select_by_location_hour_workday(df, "washington_dc", 8, TRUE)
dc2 = select_by_location_hour_workday(df, "washington_dc", 12, TRUE)
dc3 = select_by_location_hour_workday(df, "washington_dc", 18, TRUE)

seoul1 = select_by_location_hour_workday(df, "seoul", 8, TRUE)
seoul2 = select_by_location_hour_workday(df, "seoul", 12, TRUE)
seoul3 = select_by_location_hour_workday(df, "seoul", 18, TRUE)

joined = rbind(london1, london2, london3,
               dc1, dc2, dc3,
               seoul1, seoul2, seoul3)
joined$hr = paste(as.character(joined$hr), rep("hr", length(joined)))

title = "Number of Bike rentals conditioned on season, time and location"
subtitle = "Bike counts are normalized by the max rental count based on location."
ggplot(data=joined, mapping=aes(temp, cnt, color=season)) + 
  geom_point() + 
  facet_wrap(~ location + hr) + 
  ggtitle(title, subtitle = subtitle) +
  xlab("Temperature (in Celcius)") +
  ylab("Normalized Rental Bike Count") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"),
        plot.subtitle = element_text(hjust = 0.5),
        axis.title = element_text(face="bold"),
        text = element_text(size=50),
        panel.spacing.x = unit(25, "mm"))
```

We see that in Seoul, during different seasons, there are different trends of count against temp. In winter we observe that the gradient of count with respect to temp is small and positive, in spring and fall, it is moderately positive and strongly negative in summer. This trend is not seen in London and much less obvious in Washington D.C. This gives us motivation to use the interaction term $temp*location*season$ and all other interaction terms implied by the principle of marginality with the inclusion of this term.

We posit that this is because the annual temperature ranges in London and Washington DC is similar to that of Seoul's temperature range during Spring/Fall, because we see that the gradient of count against temp seems to be negative when temperature approaches the typical summer temperature of Seoul. However, it is difficult for us to determine a proper temperature cutoff to determine the 3 specific temperature regions, hence we think that using the season as a proxy for these different temperature categories is reasonable.

## Bike Rental Trends of Workdays vs Non-workdays

```{r, fig.width=36, fig.height=24, fig.align='center'}
location_labels <- c("seoul"="Seoul", "london"="London", "washington_dc"="Washington D.C.")

select_by_location_hour = function(df, location, hour) {
  output = df[(df$location == location) & (df$hr == hour), ]
  output$cnt = output$cnt / as.numeric(max(output$cnt))
  output$location = location_labels[output$location]
  return(output)
}

hour_label = function(hr_vec) {
  labels = c()
  for (hr in hr_vec) {
    if (hr == 0) {
      label = "12 AM"
    } else if (hr < 12) {
      label = paste0(hr, " AM")
    } else if (hr == 12) {
      label = "12 PM"
    } else {
      label = paste0(hr-12, " PM")
    }
    labels <- rbind(labels, label)

  }
  return(labels)
}

eda2_london1 = select_by_location_hour(df, "london", 8)
eda2_london2 = select_by_location_hour(df, "london", 12)
eda2_london3 = select_by_location_hour(df, "london", 18)

eda2_dc1 = select_by_location_hour(df, "washington_dc", 8)
eda2_dc2 = select_by_location_hour(df, "washington_dc", 12)
eda2_dc3 = select_by_location_hour(df, "washington_dc", 18)

eda2_seoul1 = select_by_location_hour(df, "seoul", 8)
eda2_seoul2 = select_by_location_hour(df, "seoul", 12)
eda2_seoul3 = select_by_location_hour(df, "seoul", 18)

joined2 = rbind(eda2_london1, eda2_london2, eda2_london3,
               eda2_dc1, eda2_dc2, eda2_dc3,
               eda2_seoul1, eda2_seoul2, eda2_seoul3)
joined2$hr = hour_label(joined2$hr)

ggplot(data=joined2, mapping=aes(temp, cnt, color=is_workday)) +
  geom_point(size=2.5) +
  facet_wrap(~ location + factor(hr, levels=c("8 AM", "12 PM", "6 PM"))) +
  xlab("Temperature (in Celcius)") +
  ylab("Normalized Rental Bike Count") +
  ggtitle("Number of Bike Rentals For Workdays vs. Non-Workdays", subtitle = "Bike counts are normalized by the max rental count based on location.") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"),
        plot.subtitle = element_text(hjust = 0.5),
        axis.title = element_text(face="bold"),
        text = element_text(size=50),
        panel.spacing.x = unit(25, "mm"))
```

From this visualization, we can see there is a clear separation/grouping for the number of bike rentals between working days vs non-working days during hours of the day when we can expect many people to be commuting to and from work (e.g at 8 AM and 6 PM). In all three locations, we can see that there are more bike rentals during peak work commute hours on working days compared to non-working days. However, during other times of the day when we don't expect people to be commuting to and from work (e.g at 12 PM), there isn't as clear of a distinction between the effects of it being a workday vs non-workday. For instance, we can observe that Seoul at 12 PM follows very similar trends for the number of bike rentals against the temperature regardless of whether it is a workday or non-workday.

This visualizaton gives us some motivation to potentially use interaction term $temp*hr*is\_workday$ along with all other relevant interaction terms implied by the principle of marginality. We will do further analysis to identify the hours in which this triple interaction term is significant. Since we noticed that some of the graphs in the extended visualization (looking at all 24 hours of the day) appear to have the same gradient between workdays and non-workdays with a potential offset depending on the hour, while others appear to have differing gradients.
