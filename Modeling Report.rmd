---
title: "Modeling Report"
author: "Benjamin Lee, Stephanie Trinh, Zhi Long Yeo"
date: "2023-05-03"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(pracma)
library(data.table)
library(dplyr)
library(tidyr)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(GGally)
library(faux)
library(janitor)
library(glmnet)
library(MASS)
library(caret)
library(RColorBrewer)
library(car)

set.seed(5)

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

# Introduction

We plan to analyze various datasets related to bike sharing data from different geographical locations (e.g. Seoul, Washington D.C., London etc) which contains information about the number of bikes rented on different days, along with weather conditions (temperature, humidity, etc) and miscellaneous information about the day these bikes were rented (weekends, holidays, etc).

## Research question

The question we wish to answer using these datasets, is what various variables/factors can be used to predict the number of bikes that will be rented on a given day.

We also want to test the following hypotheses:

-   Is $\beta_{temp}$ different in different locations? We expect people in different areas to have different temperature preferences.
-   How does $\beta_{isWorkday}$ differ in different locations?
-   Which is the most significant $\beta$? We expect it to be $\beta_{temp}$, but expect $\beta_{windspeed}$ to be significant too.
-   Does the number of bike users follow any "common" distribution? We expect it to follow a discrete probability distribution

## Practical Decisions

Our findings could help inform bike sharing companies into making better economic decisions.

i.  Marketing decisions: A better understanding of the factors that affect number of users can provide valuable insights into marketing decisions, such as discounts and promotions. Companies can offer discounts during off-peak times or inclement weather to encourage greater usage.

ii. Maintenance: Our findings could help bike sharing companies predict the expected mileage (and thus wear-and-tear) on the bikes ahead of time. It can also help them in finding the right timeslots (i.e a period where bike demand is not too high) to conduct their maintenance. Both of these factors aid them in scheduling their maintenance.

## Primary focus

Our primary focus is causal inference. We aim to figure out what are the factors most likely to affect the count of bike users.

\newpage

# Data Overview

We are using the following datasets (each bullet-point is a hyperlink) for data exploration and modeling:

-   [Bike Sharing in Washington D.C. Dataset (2011-2012)](https://www.kaggle.com/datasets/marklvl/bike-sharing-dataset)
-   [Seoul Bike Sharing Demand Data Set (2017-2018)](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#)
-   [London bike sharing dataset (2015-2017)](https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset)

Each dataset contains the hourly count of rental bikes on each specific date, with additional information on weather and holiday schedules. Each observation corresponds to an hour of the day, resulting in observations being dependent on each other. Working with time series data will be one of the challenges of working with these datasets for linear modeling, but we plan to lessen the effects of dependence between observations by treating "hour" as a categorical variable.

It would probably be difficult for modeling on this data to be generalizable to a larger population and be applicable to other locations since the popularity of bike sharing and general trends varies across locations in a way that cannot be captured within the model. Working with these datasets will likely only provide us with a model appropriate specifically for Washington D.C, Seoul, and London.

Regarding additional features and data that we believe would be useful for analysis and modeling, we think that the inclusion of additional weather data like precipitation could be useful in improving the model. This is because it would make sense for the amount of rainfall to impact the number of people choosing to utilize rental bikes on a particular day. However, since not all the datasets we are using contain precipitation data, we are not currently planning to utilize rainfall as a feature in developing our model.

# Modeling Tool Motivations

# Modeling Tool Assumptions

# Testing GLMs

Shown below is the data that we are using for our analysis. Our data consists of 43,553 rows.

```{r loading data}
df_full = read.csv('combined_dataset_cleaned.csv', colClasses= c("integer", "character", "logical", "character", "numeric", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "character", "integer", "logical", "logical"))
df = df_full[c("cnt", "temp", "hum", "windspeed", "location", "hr", "is_workday", "season")]
df$hr = as.character(df$hr)
kable(head(df), caption = "Preview of data used")
```

We first test the various models with cross-validation to observe the errors.

```{r CV Model training}
num_folds = 10
seeds = 1:(num_folds + 1)
train_control <- trainControl(method="cv", number=num_folds, seeds=seeds)
train_control_nb <- caret::trainControl(
	method = "cv", 
	number=10, 
	seeds = list(rep(1, 3), 
				 rep(2, 3), 
				 rep(3, 3), 
				 rep(4, 3), 
				 rep(5, 3),
				 rep(6, 3),
				 rep(7, 3),
				 rep(8, 3),
				 rep(9, 3),
				 rep(10, 3),
				 11))

ols_simple_train <- train(cnt ~ ., data=df, method="lm", trControl=train_control)
ols_complex_train = train(
  cnt ~ . + temp*location*season + temp*hr*is_workday,
  data=df, 
  method="lm", 
  trControl=train_control
)

log_ols_simple_train <- train(log(1+cnt) ~ ., data=df, method="lm", trControl=train_control)
log_ols_complex_train = train(
  log(1+cnt) ~ . + temp*location*season + temp*hr*is_workday,
  data=df, 
  method="lm", 
  trControl=train_control
)

poisson_simple_train = train(
  cnt ~ ., 
  data=df, 
  method="glm", 
  family="poisson", 
  trControl=train_control
)
poisson_complex_train = train(
  cnt ~ . + temp*location*season + temp*hr*is_workday,
  data=df, 
  method="glm", 
  family="poisson", 
  trControl=train_control
)

nbm_simple_train = train(cnt ~ ., data=df, method="glm.nb", trControl=train_control_nb)
nbm_complex_train = train(
  cnt ~ . + temp*location*season + temp*hr*is_workday, 
  data=df, 
  method="glm.nb", 
  trControl=train_control_nb
)
```

```{r}
summary_train_df = data.frame(
  model = c(
    "ols_simple", 
    "ols_complex", 
    "log_ols_simple", 
    "log_ols_complex", 
    "poisson_simple", 
    "poisson_complex",
    "nbm_simple",
    "nbm_complex"
    ),
  rmse = c(
    ols_simple_train$results$RMSE,
    ols_complex_train$results$RMSE,
    sqrt(mean((exp(predict(log_ols_simple_train, newdata=df)) - 1 - df$cnt)^2)),
    sqrt(mean((exp(predict(log_ols_complex_train, newdata=df)) - 1 - df$cnt)^2)),
    poisson_simple_train$results$RMSE,
    poisson_complex_train$results$RMSE,
    nbm_simple_train$results$RMSE[2],
    nbm_complex_train$results$RMSE[2]
  ),
  mae = c(
    ols_simple_train$results$MAE,
    ols_complex_train$results$MAE,
    mean(abs((exp(predict(log_ols_simple_train, newdata=df)) - 1 - df$cnt))),
    mean(abs((exp(predict(log_ols_complex_train, newdata=df)) - 1 - df$cnt))),
    poisson_simple_train$results$MAE,
    poisson_complex_train$results$MAE,
    nbm_simple_train$results$MAE[2],
    nbm_complex_train$results$MAE[2]
  ),
  rSquared = c(
    ols_simple_train$results$Rsquared,
    ols_complex_train$results$Rsquared,
    log_ols_simple_train$results$Rsquared,
    log_ols_complex_train$results$Rsquared,
    poisson_simple_train$results$Rsquared,
    poisson_complex_train$results$Rsquared,
    nbm_simple_train$results$Rsquared[2],
    nbm_complex_train$results$Rsquared[2]
  )
)

kable(summary_train_df, caption="CV Train Results")
```

We observe that the models with the triple interaction terms perform better (on RMSE) than the simpler models. This follows our expectations from our EDA where we see different trends when we conditioned on these triple interaction terms. We also see that the RMSE of the poisson model is better than the negative binomial model, which is interesting. This will be explored in the following section.

We train on full data in the following section.

```{r Comparing full models}
ols_simple = lm(cnt ~ ., data=df)
ols_complex = lm(cnt ~ . + temp*location*season + temp*hr*is_workday, data=df)
 
log_ols_simple = lm(log(1+cnt) ~ ., data=df)
log_ols_complex = lm(log(1+cnt) ~ . + temp*location*season + temp*hr*is_workday, data=df)

poisson_simple = glm(cnt ~ ., data = df, family = "poisson")
poisson_complex = glm(cnt ~ . + temp*location*season + temp*hr*is_workday, data = df, family = "poisson")

nbm_simple = glm.nb(cnt ~ ., data = df, link="log")
nbm_complex = glm.nb(cnt ~ . + temp*location*season + temp*hr*is_workday, data = df, link="log")
```

```{r}
models = c(
  "ols_simple", 
  "ols_complex", 
  "log_ols_simple", 
  "log_ols_complex", 
  "poisson_simple", 
  "poisson_complex",
  "nbm_simple",
  "nbm_complex"
)

rmses = c(
  sqrt(mean((ols_simple$fitted.values - df$cnt)^2)),
  sqrt(mean((ols_complex$fitted.values - df$cnt)^2)),
  sqrt(mean(((exp(log_ols_simple$fitted.values) - 1) - df$cnt)^2)),
  sqrt(mean(((exp(log_ols_complex$fitted.values) - 1) - df$cnt)^2)),
  sqrt(mean((poisson_simple$fitted.values - df$cnt)^2)),
  sqrt(mean((poisson_complex$fitted.values - df$cnt)^2)),
  sqrt(mean((nbm_simple$fitted.values - df$cnt)^2)),
  sqrt(mean((nbm_complex$fitted.values - df$cnt)^2))
)

maes = c(
  mean(abs(ols_simple$residuals)),
  mean(abs(ols_complex$residuals)),
  mean(abs(((exp(log_ols_simple$fitted.values) - 1) - df$cnt))),
  mean(abs(((exp(log_ols_complex$fitted.values) - 1) - df$cnt))),
  mean(abs((poisson_simple$fitted.values - df$cnt))),
  mean(abs((poisson_complex$fitted.values - df$cnt))),
  mean(abs((nbm_simple$fitted.values - df$cnt))),
  mean(abs((nbm_complex$fitted.values - df$cnt)))
)

loglikelihoods = c(
  logLik(ols_simple),
  logLik(ols_complex),
  logLik(log_ols_simple),
  logLik(log_ols_complex),
  logLik(poisson_simple),
  logLik(poisson_complex),
  logLik(nbm_simple),
  logLik(nbm_complex)
)

summary_df = data.frame(
  model = models, rmse = rmses, mae = maes, loglikelihood = loglikelihoods
)
kable(summary_df, caption="Full Data Train Results")
```


Again, we see that when training on full data, the RMSE (and log-likelihood) of the complex models are better than that of the simpler models. An interesting observation is that even though the RMSE of the poisson models are better than the equivalent negative binomial models, they have much worse log-likelihoods, approx 10x worse. Due to the extremely poor log-likelihood of the poisson model, we will not be using it for further analysis, although we will still investigate the fit of this model.

The best performing model based on log-likelihood is the log-ols model, by a significant margin. It also has the best RMSE and MAE of the models excluding the poisson models. This provides us justification for choosing this over the other models in our further analysis. The following plots show us the relationship between predicted values and residuals. We observe that residuals are not evenly scattered about 0.

```{r}
myPalette = colorRampPalette(rev(brewer.pal(11, "RdYlGn")))

generate_df = function(model) {
  output_df = data.frame(preds=model$fitted.values, residuals=model$fitted.values - df$cnt)
  return(output_df)
}

generate_plot = function(model, title) {
  resids_df = generate_df(model)
  plot = ggplot(data=resids_df, aes(x=preds, y=residuals, colour=df$cnt)) + 
    geom_point(size=0.3, alpha=0.8) +
    ggtitle(title) +
    scale_colour_gradientn(colours = myPalette(100)) +
    theme(plot.title = element_text(hjust = 0.5, size=10))
  return(plot)
}

generate_df_log_models = function(model) {
  preds = predict(model, newdata=df)
  resids = exp(preds) - 1 - df$cnt
  output_df = data.frame(preds=exp(preds) - 1, residuals=resids)
  return(output_df)
}

generate_plot_log_models = function(model, title) {
  resids_df = generate_df_log_models(model)
  plot = ggplot(data=resids_df, aes(x=preds, y=residuals, colour=df$cnt)) + 
    geom_point(size=0.3, alpha=0.8) +
    ggtitle(title) +
    scale_colour_gradientn(colours = myPalette(100)) +
    theme(plot.title = element_text(hjust = 0.5, size=10))
  return(plot)
}

ggarrange(
  generate_plot(ols_simple, "ols_simple"),
  generate_plot_log_models(log_ols_simple, "log_ols_simple"),
  generate_plot(poisson_simple, "poisson_simple"),
  generate_plot(nbm_simple, "nbm_simple"),
  generate_plot(ols_complex, "ols_complex"),
  generate_plot_log_models(log_ols_complex, "log_ols_complex"),
  generate_plot(poisson_complex, "poisson_complex"),
  generate_plot(nbm_complex, "nbm_complex"),
  nrow=2,
  ncol=4,
  common.legend = TRUE
)
```

Here's a closer look at the log-ols model, with the fit against $cnt$ and $log(1+cnt)$ respectively. We observe that besides a group of errors when $cnt$ is extremely low (seen by the negative downwards sloping lines), the log-ols model has a good fit against $log(cnt+1)$. We see that when $cnt$ is extremely low, our model frequently overpredicts the actual value. Our residuals are evenly scattered around the x-axis for our log-normal model, with slight heteroskedasticity.

```{r}
generate_df_log_models_logcnt = function(model) {
  preds = predict(model, newdata=df)
  output_df = data.frame(preds=preds, residuals=model$residuals)
  return(output_df)
}

generate_plot_log_models_logcnt = function(model, title) {
  resids_df = generate_df_log_models_logcnt(model)
  plot = ggplot(data=resids_df, aes(x=preds, y=residuals, colour=df$cnt)) + 
    geom_point(size=0.3, alpha=0.8) +
    ggtitle(title) +
    scale_colour_gradientn(colours = myPalette(100)) +
    theme(plot.title = element_text(hjust = 0.5, size=10))
  return(plot)
}
ggarrange(
  plot_log_complex_cnt = generate_plot_log_models(log_ols_complex, "log_ols_complex_cnt"),
  plot_log_simple_cnt = generate_plot_log_models(log_ols_simple, "log_ols_simple_cnt"),
  plot_log_complex_logcnt = generate_plot_log_models_logcnt(log_ols_complex, "log_ols_complex_logcnt"),
  plot_log_simple_logcnt = generate_plot_log_models_logcnt(log_ols_simple, "log_ols_complex_logcnt"),
  nrow=2,
  ncol=2,
  common.legend = TRUE
)


```

# Model Selection

In the following, we apply both AIC and BIC model selection (both backwards and forwards) to select for a sparser set of variables from the log-ols model. We expect BIC to give us a sparser model because the penalty term of BIC scales with the number of parameters in the model, but AIC does not. Note that both AIC and BIC require a likelihood distribution of our coefficient vector, $\beta$, hence the methods we are currently applying are valid only if our assumption that our data follows a log normal distribution is true.

```{r, echo=FALSE, results='hide', message=FALSE}
full.model = lm(log(cnt + 1) ~ . + temp*location*season + temp*hr*is_workday, data = df)

bic_step_forward.model = stepAIC(
  full.model, 
  direction = "forward",
  k = log(nrow(df)),
  trace = TRUE
)
aic_step_forward.model = stepAIC(
  full.model, 
  direction = "forward",
  trace = TRUE
)

bic_step_backward.model = stepAIC(
  full.model, 
  direction = "backward",
  k = log(nrow(df)),
  trace = TRUE
)
aic_step_backward.model = stepAIC(
  full.model, 
  direction = "backward",
  trace = TRUE
)

bic_step_both.model = stepAIC(
  full.model, 
  direction = "both",
  k = log(nrow(df)),
  trace = TRUE
)
aic_step_both.model = stepAIC(
  full.model, 
  direction = "both",
  trace = TRUE
)
```
```{r}
num_coeffs_df = data.frame(
  model = c("full", 
            "bic_forward", 
            "aic_forward", 
            "bic_backward", 
            "aic_backward", 
            "bic_bidirectional", 
            "aic_bidirectional"),
  num_coeff = c(
    size(full.model$coefficients)[2],
    size(bic_step_forward.model$coefficients)[2],
    size(aic_step_forward.model$coefficients)[2],
    size(bic_step_backward.model$coefficients)[2],
    size(aic_step_backward.model$coefficients)[2],
    size(bic_step_both.model$coefficients)[2],
    size(aic_step_both.model$coefficients)[2]
  )
)
kable(num_coeffs_df, caption="Number of coefficients after selection")

generate_errors = function(model) {
  preds = exp(predict(model, df)) - 1
  actuals = df$cnt
  return(data.frame(rmse=sqrt(mean((preds - actuals)^2)), mae=mean(abs(preds - actuals))))
}

full_errors = generate_errors(full.model)
reduced_model_errors = generate_errors(aic_step_both.model)

kable(data.frame(
  model=c("full", "reduced"), 
  rmse=c(full_errors$rmse, reduced_model_errors$rmse),
  mae=c(full_errors$mae, reduced_model_errors$mae)
  ),
  caption="Errors of models")
```

Forward selection does not shrink the model at all, while backwards and bidirectional selection shrinks both the AIC and BIC model by the same 23 parameters. The parameters removed are:

```{r}
all_coeff_names = names(full.model$coefficients)
bic_coeff_names_both = names(bic_step_both.model$coefficients)
aic_coeff_names_both = names(aic_step_both.model$coefficients)
setdiff(all_coeff_names, bic_coeff_names_both)
```
The selection criteria means that the triple interaction term of $temp*hr*isWorkday$ is removed. The resulting model is:

```{r}
summary(bic_step_both.model)
```

Most of our covariates have very significant coefficients, but we note that the following covariates do not have significant p-values. We will run a F-test to determine if they are significant as a group. 

```{r}
pvals = summary(bic_step_both.model)$coefficients[, 4]
pvals = pvals[pvals >= 0.1]
kable(data.frame(pvals), caption="Insignificant Coefficients")
```


We also test variable selection with LASSO. Choosing $\lambda_{1\sigma}$, we find that lasso shrinks the model by the following 48 variables.

```{r}
X = model.matrix(full.model)
y = log(1 + df$cnt)
cv_model = cv.glmnet(X, y, alpha=1, standardize = TRUE)
lasso_1se_model = glmnet(x=X, y=y, lambda = cv_model$lambda.1se, standardize = TRUE)
zeroed_variables = c()
for (x in 1:120) {
  if (lasso_1se_model$beta[x] == 0) {
    zeroed_variables = c(zeroed_variables, x)
  }
}
row.names(lasso_1se_model$beta)[zeroed_variables]
```
However, this model shrinkage does not result in an easily understood model, and as seen below, the errors are significantly worse than the errors of the models selected by AIC/BIC. We will therefore not use this model.

```{r}
preds = predict(lasso_1se_model, newx=X)
actuals = y
resids = preds - actuals
rmse = sqrt(mean((exp(preds) - 1 - actuals)^2))
mae = mean(abs(exp(preds) - 1 - actuals))

kable(data.frame(rmse=rmse, mae=mae), caption="Errors of LASSO 1se model")
```


## Bootstrap F-test

Due to heteroskedasticity in our data, we will be employing the bootstrap F-test to answer our research questions.

```{r}
getFStat <- function(model, X, L, c) {
    X = as.matrix(X)
    ## NOTE 1: beta_naut must be only the values 
    ##        of the params of interest for the hypothesis test.
    ## NOTE 2: X must be (IN THE PROPER ORDER) the 
    ##        numeric columns of the regression 
    ##        (that you made in part (b), for instance)
    
    mod_summary <- summary(model)
    
    hat_beta <- mod_summary$coefficients[,1] %>% as.matrix(ncol=1)
    
    F_num <- 1/nrow(L) * t(L %*% hat_beta - c) %*% 
        solve(L %*% solve(t(X) %*% X) %*% t(L)) %*% 
        (L %*% hat_beta - c)
    
    F_denom <- sum(residuals(model)^2) / (nrow(X) - nrow(hat_beta)) 
    
    return(F_num / F_denom)
}
```

```{r}
get_p_value_from_bootstrap = function(model, L, c) {
  B = 1000
  X = model.matrix(model)
  coefficients = names(model$coefficients)
  log_cnt = log(df$cnt + 1)
  X_plus_log_cnt = cbind(X, log_cnt = log_cnt)
  estimated_c = L %*% summary(model)$coefficients[,1] %>% as.matrix(ncol=1)
  f_score_to_test = getFStat(model, X, L, c)
  for (i in 1:B) {
    set.seed(i)
    random_rows = sample(1:n, n, replace=TRUE)
    X_plus_log_cnt_random_sample = X_plus_log_cnt[random_rows, ]
    lm_bootstrap = lm(log_cnt ~ ., data=as.data.frame(X_plus_log_cnt_random_sample))
    X_random_sample = subset(X_plus_log_cnt_random_sample, select=-(log_cnt)) # Remove log_cnt
    bootstrapped_f = getFStat(lm_bootstrap, X_random_sample, L, estimated_c)
    vect_of_bootstrapped_f_scores[i] = bootstrapped_f
  }
  get_p_value_from_bootstrap = sum(vect_of_bootstrapped_f_scores > f_score_to_test[1]) / B
}
```

# Question 1: Is $\beta_{temp}$ different in different locations? We expect people in different areas to have different temperature preferences.
To test this hypothesis, we want to test whether the betas for temp:locationseoul and temp:locationwashington_dc are 0.

```{r}
# Get the indices of temp:locationseoul and temp:locationwashington_dc
coefficients = names(bic_step.model$coefficients)
idx_temp_seoul = which(coefficients == "temp:locationseoul") #34
idx_temp_wash = which(coefficients == "temp:locationwashington_dc") #35

L_temp_location = matrix(0, nrow=2, ncol=97)
L_temp_location[1, idx_temp_seoul] = 1
L_temp_location[2, idx_temp_wash] = 1
# linearHypothesis(bic_step.model, L_temp_location)
# X_bic = model.matrix(bic_step.model)
# f_score_to_test_temp_location = getFStat(bic_step.model, X_bic, L_temp_location, matrix(c(0, 0), ncol=1))
p_value_temp_location = get_p_value_from_bootstrap(bic_step.model, L_temp_location, matrix(c(0, 0), ncol=1))
p_value_temp_location
```
Since the p value from bootstrap is very close to 0, we see that the interaction term temp*location is significant, proving that our hypothesis was right.
